{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c276e922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyvarner/mambaforge/envs/dev/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/andyvarner/mambaforge/envs/dev/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <E7E99FB4-837B-39DF-9112-617A7DBD769D> /Users/andyvarner/mambaforge/envs/dev/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <A3DB6B6A-7FF5-3593-A8F9-22FFB1D3923A> /Users/andyvarner/mambaforge/envs/dev/lib/python3.8/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn #oop\n",
    "import torch.nn.functional as F #functions\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from sklearn import datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn.metrics as sk_m\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "from IPython import embed\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb548782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pytorch_Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data, transforms):\n",
    "        \n",
    "        super(Pytorch_Dataset, self).__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.create_dataset()\n",
    "        \n",
    "    def create_dataset(self):\n",
    "        \n",
    "        self.dataset = []\n",
    "        for sample, label in zip(self.data.samples, self.data.labels):\n",
    "            self.dataset.append([sample, label])\n",
    "\n",
    "    def transform_samples(self, samples):\n",
    "        \n",
    "        return self.transforms(samples)\n",
    "    \n",
    "    def transform_labels(self, labels):\n",
    "        \n",
    "        return torch.tensor(labels)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # do transforms here :)\n",
    "        \n",
    "        sample, label = self.dataset[index]\n",
    "        \n",
    "        if(not(isinstance(sample, torch.Tensor))):\n",
    "            sample = self.transform_samples(sample).float()\n",
    "            \n",
    "        if(not(isinstance(label, torch.Tensor))):\n",
    "            label = self.transform_labels(label).float()\n",
    "        \n",
    "        return sample, label                            \n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64fe0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    \n",
    "    if(os.path.exists(path)):  # if the path already exits\n",
    "        #shutil.rmtree(path)  # remove the folder\n",
    "        return\n",
    "    else:\n",
    "        os.makedirs(path) # create the folder\n",
    "        \n",
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "\n",
    "def one_hot_encode(all_labels, num_classes):\n",
    "    \n",
    "    ohv_labels = np.zeros((len(all_labels), num_classes))\n",
    "    \n",
    "    for i, current_label in enumerate(all_labels):\n",
    "        ohv_labels[i][current_label] = 1\n",
    "        \n",
    "    return ohv_labels\n",
    "    \n",
    "def get_dataset(which_dataset):\n",
    "    \n",
    "    if(which_dataset == \"mnist\".lower()):\n",
    "        \n",
    "        train = datasets.MNIST(root='../DATA', train=True, download=True)\n",
    "        test = datasets.MNIST(root='../DATA', train=False, download=True)\n",
    "        \n",
    "    if(which_dataset == \"fashion\".lower()):\n",
    "        \n",
    "        train = datasets.FashionMNIST(root='../DATA', train=True, download=True)\n",
    "        test = datasets.FashionMNIST(root='../DATA', train=False, download=True)\n",
    "    \n",
    "    train.data, test.data = train.data.numpy(), test.data.numpy()\n",
    "    train.targets, test.targets = train.targets.numpy(), test.targets.numpy()\n",
    "    \n",
    "    # use one hot encoding for MSE - turn off for torch.nn.CrossEnropy()\n",
    "    train.targets = one_hot_encode(train.targets, 10)\n",
    "    test.targets = one_hot_encode(test.targets, 10)\n",
    "    \n",
    "    train, test = Dataset(train.data, train.targets), Dataset(test.data, test.targets)\n",
    "    \n",
    "    sample_transforms = transforms.Compose([transforms.ToTensor(), \n",
    "                                           transforms.Normalize((0.5), (0.5))])\n",
    "    \n",
    "    train, test = Pytorch_Dataset(train, sample_transforms), Pytorch_Dataset(test, sample_transforms)\n",
    "    \n",
    "    # batch_size: how many samples at a time we pass to the model (generally 8 to 64). helps us generalize (each time it optimizes, optimizations that \"stick around\" tend to be general, rather than overfit, features). also helps training time\n",
    "    # shuffle: helps with generalization. don't want to learn all 1s, then all 2s, and so on.\n",
    "    train = torch.utils.data.DataLoader(train, batch_size=16, shuffle=True)\n",
    "    test = torch.utils.data.DataLoader(test, batch_size=1, shuffle=False)\n",
    "                                           \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6815483",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module): #inherits from nn.Module\n",
    "    \n",
    "    def __init__(self, lr, num_features, num_classes, loss, network, kernel_size, num_filters):\n",
    "        \n",
    "        super(Net, self).__init__() #initialize nn.Module\n",
    "        \n",
    "        self.alpha = lr\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.loss_choice = loss\n",
    "        self.network = network\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_filters = num_filters\n",
    "            \n",
    "        # MLP\n",
    "        if(self.network.lower() == \"mlp\"):\n",
    "            self.network = torch.nn.Sequential(torch.nn.Linear(num_features, 512),\n",
    "                                               torch.nn.ReLU(),\n",
    "                                               torch.nn.Linear(512, 256),\n",
    "                                               torch.nn.ReLU(),\n",
    "                                               torch.nn.Linear(256, 128),\n",
    "                                               torch.nn.ReLU(),\n",
    "                                               torch.nn.Linear(128, 64),\n",
    "                                               torch.nn.ReLU(),\n",
    "                                               torch.nn.Linear(64, num_classes))\n",
    "#             self.network = torch.nn.Sequential(torch.nn.Linear(num_features, 64),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(64, 64),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(64, 64),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(64, 64),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(64, 64),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(64, 64),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(64, 64),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(64, 64),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(64, 32),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(32, 32),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(32, 32),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(32, 32),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(32, 32),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(32, 32),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(32, 16),\n",
    "#                                                torch.nn.ReLU(),\n",
    "#                                                torch.nn.Linear(16, num_classes))\n",
    "        # CNN\n",
    "        if(self.network.lower() == \"cnn\"):\n",
    "            # experiment 2 has kernel size 5\n",
    "            self.extract = torch.nn.Sequential(nn.Conv2d(in_channels = 1, out_channels = 2, kernel_size = kernel_size, stride = 1, padding = 1),\n",
    "                                               nn.Sigmoid(),\n",
    "                                               nn.MaxPool2d(2),\n",
    "                                               nn.Conv2d(in_channels = 2, out_channels = 4, kernel_size = kernel_size, stride = 1, padding = 1),\n",
    "                                               nn.Sigmoid(),\n",
    "                                               nn.MaxPool2d(2),\n",
    "                                               \n",
    "                                                )\n",
    "            # in_channels = 1 for grayscale\n",
    "            # out_channels -> number of shared weights / features\n",
    "            # kernel size -> nxn kernel size\n",
    "            # stride -> how many pixels we move at a time\n",
    "            # padding -> adds 1 pixesl of zeros to each side of each dimension to maintain spatial dimensions for our kernel size\n",
    "            self.decimate = torch.nn.Sequential(nn.Linear(4 * (7*7), 12),\n",
    "                                                nn.Sigmoid(),\n",
    "                                                nn.Linear(12, num_classes)\n",
    "                                                )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # RBFN\n",
    "        \n",
    "        \n",
    "        \n",
    "    def init_optimizer(self):\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = self.alpha)\n",
    "        \n",
    "        #self.optimizer = torch.optim.SGD(self.parameters(), lr = self.alpha)\n",
    "\n",
    "    def objective(self, preds, labels): # this is the loss function\n",
    "        \n",
    "        #preds = F.log_softmax(preds, dim = 1) # dim 1: distribute across output layer of tensors. like np's axis param\n",
    "    \n",
    "        loss = torch.nn.MSELoss()\n",
    "        \n",
    "        return loss(preds, labels)\n",
    "    \n",
    "    def forward(self, x): # you can complicate the network in the forward() method.\n",
    "        \n",
    "        if(self.network.lower() == \"mlp\"):\n",
    "            x = self.network(x)\n",
    "        \n",
    "            return x\n",
    "        \n",
    "        if(self.network.lower() == 'cnn'):\n",
    "            features = self.extract(x)\n",
    "            features = features.view(features.size()[0], -1)\n",
    "            output = self.decimate(features)\n",
    "            \n",
    "            return output\n",
    "    \n",
    "def train(model, train_dataset, test_dataset, num_epochs = 50, rate = 5):\n",
    "    \n",
    "    training_loss, cf_matrices = [], []\n",
    "\n",
    "    model.init_optimizer()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Train network\n",
    "\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, data, in enumerate(tqdm(train_dataset, desc = \"Train Epoch %s\" % epoch)):\n",
    "            \n",
    "            sample, label = data\n",
    "            \n",
    "            # for CNN:\n",
    "            sample = sample.type('torch.FloatTensor')\n",
    "            label = label.type('torch.LongTensor')\n",
    "            \n",
    "            sample = sample.to(\"mps\")\n",
    "            label = label.to(\"mps\")\n",
    "            \n",
    "            # only for MLP:\n",
    "            #sample = sample.view(-1, 784)\n",
    "            \n",
    "            preds = model(sample)\n",
    "            \n",
    "            loss = model.objective(preds, label.float())\n",
    "\n",
    "            epoch_loss = epoch_loss + loss.item()\n",
    "\n",
    "            model.optimizer.zero_grad() # zero the gradients after every batch\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            model.optimizer.step() # adjust the weights\n",
    "\n",
    "        epoch_loss = epoch_loss / (i + 1)\n",
    "        \n",
    "        print(epoch_loss)\n",
    "\n",
    "        training_loss.append(epoch_loss)\n",
    "\n",
    "        # Validate network\n",
    "        \n",
    "        if(epoch % rate == 0):\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            acc = 0\n",
    "            all_labels, all_preds = [], []\n",
    "            for i, (sample, label) in enumerate(tqdm(test_dataset, desc = \"Test Epoch %s\" % epoch)):\n",
    "                \n",
    "                #sample = sample.view(-1, 784)\n",
    "                \n",
    "                sample = sample.to(\"mps\")\n",
    "\n",
    "                logits = model(sample)\n",
    "                pred = torch.argmax(logits)\n",
    "\n",
    "                label = np.argmax(label.numpy())\n",
    "                \n",
    "                all_preds.append(int(pred.detach().cpu().numpy()))\n",
    "                all_labels.append(label)\n",
    "                \n",
    "                \n",
    "                if(pred == label):\n",
    "                    acc += 1\n",
    "                    \n",
    "            acc = acc / (i + 1)\n",
    "            \n",
    "            print(\"Valid Accuracy %s\" % acc)\n",
    "                \n",
    "            ##get metrics\n",
    "            training_metrics = {}\n",
    "            cf_matrix = sk_m.confusion_matrix(all_labels, all_preds)\n",
    "            \n",
    "            #epoch_accuracy = calculate_accuracy(np.asarray(all_preds), np.asarray(all_labels))\n",
    "\n",
    "            cf_matrices.append(cf_matrix)\n",
    "            print(f\"confusion matrix appended. epoch {epoch}\")\n",
    "            model.train()\n",
    "            \n",
    "        training_metrics = {}\n",
    "        training_metrics[\"labels\"] = all_labels\n",
    "        training_metrics[\"preds\"] = all_preds\n",
    "        training_metrics[\"mats\"] = cf_matrices\n",
    "            \n",
    "    return training_loss, training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d1a003",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m     Xs, ys \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m ys:\n\u001b[0;32m---> 29\u001b[0m         counter[\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(counter)\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "which_dataset = \"mnist\" # \"mnist\" or \"fashion\"\n",
    "which_network = \"cnn\"\n",
    "\n",
    "loss_choice = \"mse\"\n",
    "\n",
    "batch_size = 16\n",
    "num_features = 784\n",
    "num_classes = 10\n",
    "\n",
    "alpha = 1e-4 \n",
    "\n",
    "kernel_size = 3\n",
    "num_filters = 8\n",
    "\n",
    "train_dataset, test_dataset = get_dataset(which_dataset)\n",
    "\n",
    "model = Net(alpha, num_features, num_classes, loss_choice, which_network, kernel_size, num_filters).to(\"mps\")\n",
    "\n",
    "#train_loss, train_metrics = train(model, train_dataset, test_dataset)\n",
    "# for data in train_dataset:\n",
    "#     print(data)\n",
    "#     break\n",
    "total = 0\n",
    "counter = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in train_dataset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter[int(y)] += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3416fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data[0][0], data[1][0] # tensor of tensor, image. then tensor of tensors, labels\n",
    "\n",
    "plt.imshow(x.view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_num = 4\n",
    "\n",
    "results = {}\n",
    "results[\"loss\"] = train_loss\n",
    "results[\"validation\"] = train_metrics # 1 confusion matrix per validation run\n",
    "    \n",
    "path_save = f\"/Users/andyvarner/Documents/NN_Spring2023/project_1/results/{which_network.upper()}/{which_dataset}\"\n",
    "\n",
    "if(os.path.join(path_save)):\n",
    "    print(\"creating folder\")\n",
    "    create_folder(path_save)  \n",
    "\n",
    "title = \"%s.pkl\" % (str(exp_num).zfill(3))\n",
    "filename = os.path.join(path_save, title)\n",
    "\n",
    "pickle.dump(results, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef363f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f583972",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.state_dict)\n",
    "\n",
    "torch.save(model.state_dict, \"experiment_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(\"experiment_1.pt\")\n",
    "new_model = Net(parms)\n",
    "new_model.load_state_dict(state) # loads all the weights from the trained model back into the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c403f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the weights\n",
    "print(state.keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
