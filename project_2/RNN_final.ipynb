{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1240066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import embed\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F #functions\n",
    "from torchvision import datasets, transforms\n",
    "import sklearn.metrics as sk_m\n",
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"/Users/andyvarner/Documents/NN_Spring2023/project_2\"))\n",
    "import dataset_methods\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e376be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessRNN(torch.nn.Module): #inherits from nn.Module\n",
    "    \n",
    "    #def __init__(self, input_size, hidden_size, num_layers, lr, output_size):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, num_layers, lr, nonlinearity='relu'):\n",
    "        # hidden_dim = the number of features in the hidden state h\n",
    "        # num_layers = Number of recurrent layers. \n",
    "        #              E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked \n",
    "        #              RNN, with the second RNN taking in outputs of the first RNN and computing the final \n",
    "        #              results. Default: 1\n",
    "        # nonlinearity = The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n",
    "        \n",
    "        super(ChessRNN, self).__init__() #initialize nn.Module\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, num_layers=num_layers, batch_first=True,)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self, x, num_turns):\n",
    "        \n",
    "        embed();exit()\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        out = out[0][num_turns-1]\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def init_optimizer(self):\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = self.lr)\n",
    "\n",
    "#     def objective(self, outputs, labels): # this is the loss function\n",
    "\n",
    "#         loss = torch.nn.CrossEntropyLoss(outputs, labels)\n",
    "        \n",
    "#         return loss\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "    \n",
    "def train_rnn(model, train_dataset, test_dataset, num_epochs = 10, rate = 2):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    training_losses, valid_losses, train_labels, train_preds, cf_matrices = [], [], [], [], []\n",
    "\n",
    "    model.init_optimizer()\n",
    "   \n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch_loss, train_epoch_acc, valid_epoch_loss, valid_epoch_acc = 0, 0, 0, 0\n",
    "\n",
    "        model.train()        # Set model to train mode\n",
    "\n",
    "        # Loop over the train data.\n",
    "        for i, (turns, winner, num_turns), in enumerate(tqdm(train_dataset, desc = \"Train Epoch %s\" % epoch)): ### turns = inputs, winner = labels\n",
    "\n",
    "            # Zero out gradients #\n",
    "            model.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass #\n",
    "            outputs = model(turns, num_turns)\n",
    "            \n",
    "            #loss = criterion(outputs, winner.float())\n",
    "            loss = criterion(outputs, torch.argmax(winner, dim=1))  # is this actually correct?\n",
    "            \n",
    "            # Backward pass #\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "            \n",
    "            # update training loss #\n",
    "            train_epoch_loss += loss.item()\n",
    "            \n",
    "            # Update training accuracy\n",
    "            train_pred = F.softmax(outputs, dim=1)\n",
    "            _, train_pred_labels = torch.max(train_pred, dim=1)\n",
    "            train_epoch_acc += torch.sum(train_pred_labels == torch.argmax(winner, dim=1)).item() \n",
    "            \n",
    "#             train_preds.append(winner.numpy())\n",
    "#             train_labels.append(winner)\n",
    "\n",
    "\n",
    "        train_loss = train_epoch_loss / len(train_dataset)\n",
    "        train_acc = train_epoch_acc / len(train_dataset)\n",
    "        training_losses.append(train_loss)\n",
    "        train_labels.append(winner)\n",
    "        train_preds.append(train_pred_labels)\n",
    "\n",
    "        \n",
    "        ###############\n",
    "        \n",
    "        print(f\"epoch {epoch}, epoch_loss={train_loss}, epoch_acc={train_acc}\")\n",
    "\n",
    "        # Validate network\n",
    "        \n",
    "        if(epoch % rate == 0):\n",
    "            print(\"test\")\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            valid_epoch_loss, valid_epoch_acc = 0, 0\n",
    "            valid_labels, valid_preds = [], []\n",
    "\n",
    "            for i, (turns, winner, num_turns), in enumerate(tqdm(test_dataset, desc = \"Test Epoch %s\" % epoch)):   \n",
    "                \n",
    "                outputs = model(turns, num_turns)\n",
    "                \n",
    "                # Update Loss\n",
    "                loss = criterion(outputs, torch.argmax(winner, dim=1))\n",
    "                valid_epoch_loss += loss.item()\n",
    "                \n",
    "                # Update validation accuracy\n",
    "                preds = F.softmax(outputs, dim=1)          # the model's prediction\n",
    "                _, pred_labels = torch.max(preds, dim=1)   \n",
    "                valid_epoch_acc += torch.sum(pred_labels == torch.argmax(winner, dim=1)).item()\n",
    "                \n",
    "                valid_labels.append(int(torch.argmax(winner)))\n",
    "                valid_preds.append(pred_labels.item())\n",
    "                #valid_preds.append(int(torch.argmax(preds)))\n",
    "\n",
    "            valid_losses.append(valid_epoch_loss / (len(test_dataset) + 1e-6))\n",
    "            valid_acc = valid_epoch_acc / (len(test_dataset) + 1e-6)\n",
    "\n",
    "            cf_matrix = sk_m.confusion_matrix(np.asarray(valid_labels), np.asarray(valid_preds))\n",
    "            cf_matrices.append(cf_matrix)\n",
    "\n",
    "            print(\"Valid Accuracy %s Valid Loss %s\" % (valid_acc, valid_losses))\n",
    "                \n",
    "            model.train()\n",
    "    \n",
    "        ##get metrics\n",
    "            \n",
    "        metrics = {}\n",
    "        metrics[\"valid_losses\"] = valid_losses\n",
    "        metrics[\"training_losses\"] = training_losses\n",
    "        metrics[\"labels\"] = valid_labels\n",
    "        metrics[\"preds\"] = valid_preds\n",
    "        metrics[\"mats\"] = cf_matrices\n",
    "\n",
    "    #return training_loss, training_metrics\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47a3de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 0:   0%|                                                                                                   | 0/252 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:38:11) \n",
      "Type 'copyright', 'credits' or 'license' for more information\n",
      "IPython 8.11.0 -- An enhanced Interactive Python. Type '?' for help.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "In [1]:  x.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[1]: torch.Size([1, 100, 5])\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "In [2]:  x[0].shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: torch.Size([100, 5])\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "In [3]:  x[1].shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "IndexError                                Traceback (most recent call last)\n",
      "Cell In[3], line 1\n",
      "----> 1 x[1].shape\n",
      "\n",
      "IndexError: index 1 is out of bounds for dimension 0 with size 1\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "In [4]:  x[-1].shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: torch.Size([100, 5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters - Don't change these\n",
    "input_size = 5    # number of features in the input (shape of `moves` encoding)\n",
    "output_size = 2   # number of output classes ([white, black], 0-loser, 1-winner)\n",
    "\n",
    "# hidden_dim = the number of features in the hidden state h\n",
    "# num_layers = Number of recurrent layers. \n",
    "#              e.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked \n",
    "#              RNN, with the second RNN taking in outputs of the first RNN and computing the final \n",
    "#              results. Default: 1\n",
    "# nonlinearity = The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n",
    "\n",
    "# These can change\n",
    "hidden_dim =  32 # number of hidden units in the RNN\n",
    "rate = 2 # how often we run the validation loop\n",
    "num_layers = 4  # number of recurrent layers\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "model = ChessRNN(input_size, output_size, hidden_dim, num_layers, learning_rate)\n",
    "\n",
    "filename = \"games_10.pkl\"   # the integer value is the number of moves we're looking at\n",
    "\n",
    "train, test = dataset_methods.get_dataset(filename)\n",
    "# embed()\n",
    "\n",
    "train_metrics = train_rnn(model, train, test, num_epochs = num_epochs, rate = rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c23fe6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder already exists at /Users/andyvarner/Documents/NN_Spring2023/project_2/Results5/epochs_10\n"
     ]
    }
   ],
   "source": [
    "path_save = f\"/Users/andyvarner/Documents/NN_Spring2023/project_2/Results5/epochs_{num_epochs}\"\n",
    "\n",
    "if not os.path.exists(path_save):\n",
    "    os.makedirs(path_save)\n",
    "    print(f\"Folder created at {path_save}\")\n",
    "else:\n",
    "    print(f\"Folder already exists at {path_save}\")\n",
    "\n",
    "title = \"%s\" % (str(filename).zfill(3))\n",
    "file_save = os.path.join(path_save, title)\n",
    "\n",
    "pickle.dump(train_metrics, open(file_save, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a522bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
