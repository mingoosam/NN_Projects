{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1240066a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyvarner/mambaforge/envs/dev/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/andyvarner/mambaforge/envs/dev/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <E7E99FB4-837B-39DF-9112-617A7DBD769D> /Users/andyvarner/mambaforge/envs/dev/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <A3DB6B6A-7FF5-3593-A8F9-22FFB1D3923A> /Users/andyvarner/mambaforge/envs/dev/lib/python3.8/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import embed\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F #functions\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import sklearn.metrics as sk_m\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"/Users/andyvarner/Documents/NN_Spring2023/project_2\"))\n",
    "import dataset_methods\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bcf0f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " 'd': 1,\n",
       " 'g': 2,\n",
       " 'e': 3,\n",
       " 'a': 4,\n",
       " 'n': 5,\n",
       " 'v': 6,\n",
       " 'h': 7,\n",
       " 'm': 8,\n",
       " 'o': 9,\n",
       " 'f': 10,\n",
       " 'c': 11,\n",
       " 'u': 12,\n",
       " 'w': 13,\n",
       " 'i': 14,\n",
       " 'r': 15,\n",
       " 'y': 16}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['hey how are you','good i am fine','have a nice day']\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "\n",
    "char2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edfda54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest string has 15 characters\n"
     ]
    }
   ],
   "source": [
    "maxlen = len(max(text, key=len))\n",
    "print(\"The longest string has {} characters\".format(maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88c33ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Padding\n",
    "\n",
    "# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of the sentence matches\n",
    "# the length of the longest sentence\n",
    "for i in range(len(text)):\n",
    "    while len(text[i])<maxlen:\n",
    "        text[i] += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b3e9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are yo\n",
      "Target Sequence: ey how are you\n",
      "Input Sequence: good i am fine\n",
      "Target Sequence: ood i am fine \n",
      "Input Sequence: have a nice da\n",
      "Target Sequence: ave a nice day\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Creating lists that will hold our input and target sequences\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # Remove last character for input sequence\n",
    "    input_seq.append(text[i][:-1])\n",
    "    \n",
    "    # Remove firsts character for target sequence\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41317d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea9ab40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "161ac840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (3, 14, 17) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n"
     ]
    }
   ],
   "source": [
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n",
    "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcb29a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.from_numpy(input_seq)\n",
    "target_seq = torch.Tensor(target_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536568df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58e376be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessRNN(torch.nn.Module): #inherits from nn.Module\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, lr, output_size):\n",
    "        \n",
    "        super(ChessRNN, self).__init__() #initialize nn.Module\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.lr = lr\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(hidden[-1])\n",
    "        \n",
    "        return out, hidden\n",
    "        \n",
    "    def init_optimizer(self):\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = self.lr)\n",
    "\n",
    "\n",
    "    def objective(self, outputs, labels): # this is the loss function\n",
    "\n",
    "        loss = torch.nn.CrossEntropyLoss(outputs, labels)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "    \n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "def train_rnn(model, train_dataset, test_dataset, num_epochs = 10, rate = 5):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    training_loss, cf_matrices = [], []\n",
    "\n",
    "    model.init_optimizer()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "    \n",
    "        # Set model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Loop over the train data\n",
    "        \n",
    "        for i, data, in enumerate(tqdm(train_dataset, desc = \"Train Epoch %s\" % epoch)):\n",
    "\n",
    "            turns = data[0]  #inputs\n",
    "            winner = data[1] #labels\n",
    "            \n",
    "\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, hidden = model(turns)\n",
    "\n",
    "            #loss = torch.nn.functional.mse_loss(outputs, winner)\n",
    "            loss = criterion(outputs,winner.view(-1).long())\n",
    "            \n",
    "            # Zero out gradients\n",
    "            model.optimizer.zero_grad()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            model.optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss = epoch_loss / (i + 1)\n",
    "\n",
    "        print(f\"epoch {epoch}, epoch_loss={epoch_loss}\")\n",
    "\n",
    "        training_loss.append(epoch_loss)\n",
    "\n",
    "#         # Validate network\n",
    "        \n",
    "        if(epoch % rate == 0):\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            acc = 0\n",
    "            all_labels, all_preds = [], []\n",
    "\n",
    "            for i, (turns, winner) in enumerate(tqdm(test_dataset, desc = \"Test Epoch %s\" % epoch)):\n",
    "\n",
    "#                 turns = data[0]  #inputs\n",
    "#                 winner = data[1] #labels\n",
    "                #sample = sample.view(-1, 784)\n",
    "\n",
    "                logits = model(turns)\n",
    "                pred = torch.argmax(logits)\n",
    "\n",
    "                label = np.argmax(winner.numpy())\n",
    "                \n",
    "                #all_preds.append(int(pred.detach().cpu().numpy()))\n",
    "                all_preds.append(pred.numpy())\n",
    "                all_labels.append(label)\n",
    "                \n",
    "                if(pred == label):\n",
    "                    acc += 1\n",
    "                    \n",
    "            acc = acc / (i + 1)\n",
    "            \n",
    "            print(\"Valid Accuracy %s\" % acc)\n",
    "                \n",
    "        ##get metrics\n",
    "        training_metrics = {}\n",
    "        cf_matrix = sk_m.confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        #epoch_accuracy = calculate_accuracy(np.asarray(all_preds), np.asarray(all_labels))\n",
    "\n",
    "        cf_matrices.append(cf_matrix)\n",
    "        print(f\"confusion matrix appended. epoch {epoch}\")\n",
    "        model.train()\n",
    "            \n",
    "        training_metrics = {}\n",
    "        training_metrics[\"labels\"] = all_labels\n",
    "        training_metrics[\"preds\"] = all_preds\n",
    "        training_metrics[\"mats\"] = cf_matrices\n",
    "\n",
    "    return training_loss, training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af47a3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 0:   0%|                                                                                                                                                     | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "For unbatched 2-D input, hx should also be 2-D but got 3-D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m ChessRNN(input_size, hidden_size, num_layers, learning_rate, output_size)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#train, test = get_dataset()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# embed()\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m train_loss, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 66\u001b[0m, in \u001b[0;36mtrain_rnn\u001b[0;34m(model, train_dataset, test_dataset, num_epochs, rate)\u001b[0m\n\u001b[1;32m     61\u001b[0m winner \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m#labels\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m outputs, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mturns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m#loss = torch.nn.functional.mse_loss(outputs, winner)\u001b[39;00m\n\u001b[1;32m     69\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs,winner\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mlong())\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[20], line 20\u001b[0m, in \u001b[0;36mChessRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_hidden(batch_size)\n\u001b[0;32m---> 20\u001b[0m out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out, hidden\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.8/site-packages/torch/nn/modules/rnn.py:450\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 450\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    451\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor unbatched 2-D input, hx should also be 2-D but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    452\u001b[0m         hx \u001b[38;5;241m=\u001b[39m hx\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: For unbatched 2-D input, hx should also be 2-D but got 3-D tensor"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "input_size = dict_size    # number of features in the input (one-hot encoding of moves)\n",
    "hidden_size = 12  # number of hidden units in the RNN\n",
    "output_size = dict_size   # number of output classes (1 for each player)\n",
    "num_layers = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "model = ChessRNN(input_size, hidden_size, num_layers, learning_rate, output_size)\n",
    "\n",
    "#train, test = get_dataset()\n",
    "# embed()\n",
    "\n",
    "train_loss, train_metrics = train_rnn(model, input_seq, target_seq, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"dataset_20\"\n",
    "\n",
    "results = {}\n",
    "results[\"loss\"] = train_loss\n",
    "results[\"validation\"] = train_metrics # 1 confusion matrix per validation run\n",
    "    \n",
    "path_save = f\"/Users/andyvarner/Documents/NN_Spring2023/project_2/Results\"\n",
    "\n",
    "# if(os.path.join(path_save)):\n",
    "#     print(\"creating folder\")\n",
    "#     create_folder(path_save)  \n",
    "\n",
    "title = \"%s.pkl\" % (str(exp_name).zfill(3))\n",
    "filename = os.path.join(path_save, title)\n",
    "print(filename)\n",
    "\n",
    "pickle.dump(results, open(filename, \"wb\"))\n",
    "\n",
    "# torch.save(model.state_dict(), \"kernel_size_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee798543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
