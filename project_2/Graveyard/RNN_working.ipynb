{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1240066a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andyvarner/mambaforge/envs/dev/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/andyvarner/mambaforge/envs/dev/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <E7E99FB4-837B-39DF-9112-617A7DBD769D> /Users/andyvarner/mambaforge/envs/dev/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Expected in:     <A3DB6B6A-7FF5-3593-A8F9-22FFB1D3923A> /Users/andyvarner/mambaforge/envs/dev/lib/python3.8/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython import embed\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F #functions\n",
    "from torchvision import datasets, transforms\n",
    "import sklearn.metrics as sk_m\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"/Users/andyvarner/Documents/NN_Spring2023/project_2\"))\n",
    "import dataset_methods\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "58e376be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessRNN(torch.nn.Module): #inherits from nn.Module\n",
    "    \n",
    "    #def __init__(self, input_size, hidden_size, num_layers, lr, output_size):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, num_layers, lr):\n",
    "        # hidden_dim = the number of features in the hidden state h\n",
    "        # num_layers = Number of recurrent layers. \n",
    "        #              E.g., setting num_layers=2 would mean stacking two RNNs together to form a stacked \n",
    "        #              RNN, with the second RNN taking in outputs of the first RNN and computing the final \n",
    "        #              results. Default: 1\n",
    "        # nonlinearity = The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'\n",
    "        \n",
    "        super(ChessRNN, self).__init__() #initialize nn.Module\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, num_layers=num_layers, batch_first=True,)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self, x, num_turns):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        out = out[0][num_turns-1]\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def init_optimizer(self):\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr = self.lr)\n",
    "\n",
    "#     def objective(self, outputs, labels): # this is the loss function\n",
    "\n",
    "#         loss = torch.nn.CrossEntropyLoss(outputs, labels)\n",
    "        \n",
    "#         return loss\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
    "    \n",
    "def train_rnn(model, train_dataset, test_dataset, num_epochs = 10, rate = 5):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    training_losses, valid_losses, train_labels, train_preds, cf_matrices = [], [], [], [], []\n",
    "\n",
    "    model.init_optimizer()\n",
    "   \n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch_loss, train_epoch_acc, valid_epoch_loss, valid_epoch_acc = 0, 0, 0, 0\n",
    "\n",
    "        model.train()        # Set model to train mode\n",
    "\n",
    "        # Loop over the train data.\n",
    "        for i, (turns, winner, num_turns), in enumerate(tqdm(train_dataset, desc = \"Train Epoch %s\" % epoch)): ### turns = inputs, winner = labels\n",
    "\n",
    "            # Zero out gradients #\n",
    "            model.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass #\n",
    "            outputs = model(turns, num_turns)\n",
    "            \n",
    "            #loss = criterion(outputs, winner.float())\n",
    "            loss = criterion(outputs, torch.argmax(winner, dim=1))  # is this actually correct?\n",
    "            \n",
    "            # Backward pass #\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "            \n",
    "            # update training loss #\n",
    "            train_epoch_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            # Update training accuracy\n",
    "            train_preds = F.softmax(outputs, dim=1)\n",
    "            _, train_pred_labels = torch.max(train_preds, dim=1)\n",
    "            train_epoch_acc += torch.sum(train_pred_labels == torch.argmax(winner, dim=1)).item() \n",
    "            \n",
    "#             train_preds.append(winner.numpy())\n",
    "#             train_labels.append(winner)\n",
    "\n",
    "\n",
    "        train_loss = train_epoch_loss / len(train_dataset)\n",
    "        train_acc = train_epoch_acc / len(train_dataset)\n",
    "        training_losses.append(train_loss)\n",
    "        train_labels.append(winner)\n",
    "        train_preds.append(pred_labels)\n",
    "\n",
    "        \n",
    "        ###############\n",
    "        \n",
    "        print(f\"epoch {epoch}, epoch_loss={train_loss}, epoch_acc={train_acc}\")\n",
    "\n",
    "        # Validate network\n",
    "        \n",
    "        if(epoch % rate == 0):\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            valid_epoch_loss, valid_epoch_acc = 0, 0\n",
    "            valid_labels, valid_preds = [], []\n",
    "\n",
    "            for i, (turns, winner, num_turns), in enumerate(tqdm(test_dataset, desc = \"Test Epoch %s\" % epoch)):\n",
    "                \n",
    "                outputs = model(turns, num_turns)\n",
    "                \n",
    "                loss = criterion(outputs, torch.argmax(winner, dim=1))\n",
    "                valid_epoch_loss += loss.item()\n",
    "                \n",
    "                valid_preds.append(winner.numpy())\n",
    "                valid_labels.append(winner)\n",
    "                \n",
    "                # Update validation accuracy\n",
    "                preds = F.softmax(outputs, dim=1)\n",
    "                _, pred_labels = torch.max(preds, dim=1)\n",
    "                valid_epoch_acc += torch.sum(pred_labels == torch.argmax(winner, dim=1)).item()\n",
    "                \n",
    "                valid_labels.append(torch.argmax(winner, dim=1))\n",
    "                valid_preds.append(pred_labels)\n",
    "\n",
    "            # Calculate average validation loss and accuracy\n",
    "#             print(\"figure out difference between games_20 and games_100:\")\n",
    "#             print(f\"games_100: shape: {test_dataset.shape}, type: {type(test_datset)}\")\n",
    "\n",
    "            valid_losses.append(valid_epoch_loss / (len(test_dataset) + 1e-3))\n",
    "            valid_acc = valid_epoch_acc / (len(test_dataset) + 1e-3)\n",
    "            \n",
    "            # Calculate confusion matrix\n",
    "            #cf_matrix = sk_m.confusion_matrix(torch.cat(valid_labels).numpy(), torch.cat(valid_preds).numpy())\n",
    "            #cf_matrices.append(cf_matrix)\n",
    "\n",
    "            print(\"Valid Accuracy %s\" % valid_acc)\n",
    "                \n",
    "#                 #if(pred == label):\n",
    "#                 if(torch.argmax(winner) == torch.argmax(outputs)):\n",
    "#                     valid_epoch_acc += 1\n",
    "                    \n",
    "#             valid_epoch_acc = valid_epoch_acc / (i + 1)\n",
    "            \n",
    "#             print(\"Valid Accuracy %s\" % valid_acc)\n",
    "                \n",
    "    \n",
    "                ##get metrics\n",
    "            cf_matrix = sk_m.confusion_matrix(valid_labels, valid_preds)\n",
    "            \n",
    "            #epoch_accuracy = calculate_accuracy(np.asarray(all_preds), np.asarray(all_labels))\n",
    "\n",
    "            cf_matrices.append(cf_matrix)\n",
    "            print(f\"confusion matrix appended. epoch {epoch}\")\n",
    "            model.train()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        ##get metrics\n",
    "        training_metrics = {}\n",
    "        #cf_matrix = sk_m.confusion_matrix(valid_labels, valid_preds)\n",
    "\n",
    "#         epoch_accuracy = calculate_accuracy(np.asarray(valid_preds), np.asarray(valid_labels))\n",
    "\n",
    "        #cf_matrices.append(cf_matrix)\n",
    "        #print(f\"confusion matrix appended. epoch {epoch}\")\n",
    "        model.train()\n",
    "            \n",
    "        metrics = {}\n",
    "        metrics[\"valid_losses\"] = valid_losses\n",
    "        metrics[\"training_losses\"] = training_losses\n",
    "        metrics[\"labels\"] = valid_labels\n",
    "        metrics[\"preds\"] = valid_preds\n",
    "        metrics[\"mats\"] = cf_matrices\n",
    "\n",
    "    #return training_loss, training_metrics\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af47a3de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████████████| 1385/1385 [00:09<00:00, 149.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, epoch_loss=0.44800033247105053, epoch_acc=0.8252707581227436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 0: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Accuracy 0.0\n",
      "confusion matrix appended. epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1: 100%|████████████████████████████████████████████████████████████████████████████████████| 1385/1385 [00:09<00:00, 150.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, epoch_loss=0.4164541798055387, epoch_acc=0.8382671480144405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 2: 100%|████████████████████████████████████████████████████████████████████████████████████| 1385/1385 [00:10<00:00, 133.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, epoch_loss=0.3784961918698429, epoch_acc=0.8404332129963898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 3: 100%|████████████████████████████████████████████████████████████████████████████████████| 1385/1385 [00:10<00:00, 137.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, epoch_loss=0.36582267280896646, epoch_acc=0.8469314079422383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 4: 100%|████████████████████████████████████████████████████████████████████████████████████| 1385/1385 [00:09<00:00, 142.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, epoch_loss=0.35951726650045884, epoch_acc=0.8505415162454874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 5: 100%|████████████████████████████████████████████████████████████████████████████████████| 1385/1385 [00:09<00:00, 141.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, epoch_loss=0.3559909447718291, epoch_acc=0.851985559566787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch 5: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Accuracy 0.0\n",
      "confusion matrix appended. epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 6: 100%|████████████████████████████████████████████████████████████████████████████████████| 1385/1385 [00:09<00:00, 146.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, epoch_loss=0.349296965257732, epoch_acc=0.8570397111913357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 7: 100%|████████████████████████████████████████████████████████████████████████████████████| 1385/1385 [00:09<00:00, 145.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, epoch_loss=0.34490783011897164, epoch_acc=0.8577617328519855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 8: 100%|████████████████████████████████████████████████████████████████████████████████████| 1385/1385 [00:09<00:00, 139.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, epoch_loss=0.3404392678626823, epoch_acc=0.8592057761732852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 9: 100%|████████████████████████████████████████████████████████████████████████████████████| 1385/1385 [00:09<00:00, 143.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, epoch_loss=0.33771941479066864, epoch_acc=0.8592057761732852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set hyperparameters\n",
    "input_size = 5    # number of features in the input (shape of `moves` encoding)\n",
    "output_size = 2   # number of output classes ([white, black], 0-loser, 1-winner)\n",
    "hidden_dim = 12  # number of hidden units in the RNN\n",
    "num_layers = 4\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "model = ChessRNN(input_size, output_size, hidden_dim, num_layers, learning_rate)\n",
    "\n",
    "filename = \"games_20.pkl\"\n",
    "\n",
    "train, test = dataset_methods.get_dataset(filename)\n",
    "# embed()\n",
    "\n",
    "train_metrics = train_rnn(model, train, test, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "db01f86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['valid_losses', 'training_losses', 'labels', 'preds', 'mats'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics.keys()\n",
    "#train_metrics[\"valid_losses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92340bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_metrics['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results[\"loss\"] = train_loss\n",
    "results[\"validation\"] = train_metrics # 1 confusion matrix per validation run\n",
    "    \n",
    "path_save = f\"/Users/andyvarner/Documents/NN_Spring2023/project_2/Results\"\n",
    "\n",
    "# if(os.path.join(path_save)):\n",
    "#     print(\"creating folder\")\n",
    "#     create_folder(path_save)  \n",
    "\n",
    "title = \"%s\" % (str(filename).zfill(3))\n",
    "file_save = os.path.join(path_save, title)\n",
    "\n",
    "pickle.dump(results, open(file_save, \"wb\"))\n",
    "\n",
    "# torch.save(model.state_dict(), \"kernel_size_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86661a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### GRAVEYARD ################\n",
    "\n",
    "def train_rnn(model, train_dataset, test_dataset, num_epochs = 10, rate = 5):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    training_loss, valid_loss, cf_matrices = [], [], []\n",
    "    train_labels, train_preds = [], []\n",
    "\n",
    "    model.init_optimizer()\n",
    "   \n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch_loss = 0\n",
    "        train_epoch_acc = 0\n",
    "        \n",
    "        valid_epoch_loss = 0\n",
    "        valid_epoch_acc = 0\n",
    "\n",
    "        # Set model to train mode\n",
    "        model.train()\n",
    "\n",
    "        # Loop over the train data\n",
    "        \n",
    "        for i, data, in enumerate(tqdm(train_dataset, desc = \"Train Epoch %s\" % epoch)):\n",
    "            \n",
    "            # Zero out gradients\n",
    "            model.optimizer.zero_grad()\n",
    "\n",
    "            turns = data[0]  #inputs\n",
    "            winner = data[1] #labels\n",
    "            num_turns = data[2]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(turns, num_turns)\n",
    "\n",
    "#             print(f\"pred = {outputs}\")\n",
    "#             print(f\"pred = {torch.argmax(outputs)}\")\n",
    "#             print(f\"label = {winner}\")\n",
    "#             print(f\"label = {torch.argmax(winner)}\")\n",
    "#             print(\" \")\n",
    "            \n",
    "            #loss = criterion(outputs, winner.float())\n",
    "            loss = criterion(outputs, torch.argmax(winner, dim=1))  # is this actually correct?\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            model.optimizer.step()\n",
    "\n",
    "#             ###### Update metrics  my code below:\n",
    "#             if(torch.argmax(winner) == torch.argmax(outputs)):\n",
    "#                 train_acc += 1\n",
    "                    \n",
    "#             train_acc = train_acc / (i + 1)\n",
    "            \n",
    "#             print(\"Train Accuracy %s\" % train_acc)\n",
    "\n",
    "#            train_epoch_loss += loss.item()\n",
    "\n",
    "#         train_loss = train_epoch_loss / (i + 1)\n",
    "\n",
    "#         print(f\"epoch {epoch}, epoch_loss={train_loss}\")\n",
    "        ###############\n",
    "        # Update metrics ######### chatGPT code below:\n",
    "            train_epoch_acc += torch.sum(torch.argmax(outputs, dim=1) == torch.argmax(winner, dim=1)).item()\n",
    "            \n",
    "            # update training loss\n",
    "            train_epoch_loss += loss.item()\n",
    "            \n",
    "            # Update training accuracy\n",
    "            preds = F.softmax(outputs, dim=1)\n",
    "            _, pred_labels = torch.max(preds, dim=1)\n",
    "            train_epoch_acc += torch.sum(pred_labels == torch.argmax(winner, dim=1)).item()\n",
    "\n",
    "        #train_loss = train_epoch_loss / (i + 1)\n",
    "        train_acc = train_acc / ((i + 1) * train_dataset.batch_size)\n",
    "\n",
    "        training_loss.append(train_loss)\n",
    "        \n",
    "        ###############\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        train_loss = train_epoch_loss / len(train_dataset)\n",
    "        train_acc = train_epoch_acc / len(train_dataset)\n",
    "        training_loss.append(train_loss)\n",
    "        train_labels.append(winner)\n",
    "        train_preds.append(pred_labels)\n",
    "        \n",
    "        print(f\"epoch {epoch}, epoch_loss={train_epoch_loss}, epoch_acc={train_acc}\")\n",
    "\n",
    "        # Validate network\n",
    "        \n",
    "        if(epoch % rate == 0):\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            valid_acc = 0\n",
    "            valid_labels, valid_preds = [], []\n",
    "\n",
    "            for i, data, in enumerate(tqdm(test_dataset, desc = \"Test Epoch %s\" % epoch)):\n",
    "                \n",
    "                turns = data[0]  #inputs\n",
    "                winner = data[1] #labels\n",
    "                num_turns = data[2]\n",
    "                \n",
    "                outputs = model(turns, num_turns)\n",
    "\n",
    "                valid_preds.append(winner.numpy())\n",
    "                valid_labels.append(winner)\n",
    "                \n",
    "                #if(pred == label):\n",
    "                if(torch.argmax(winner) == torch.argmax(outputs)):\n",
    "                    valid_acc += 1\n",
    "                    \n",
    "            valid_acc = valid_acc / (i + 1)\n",
    "            \n",
    "            print(\"Valid Accuracy %s\" % valid_acc)\n",
    "                \n",
    "        ##get metrics\n",
    "        training_metrics = {}\n",
    "        cf_matrix = sk_m.confusion_matrix(valid_labels, valid_preds)\n",
    "\n",
    "        epoch_accuracy = calculate_accuracy(np.asarray(valid_preds), np.asarray(valid_labels))\n",
    "\n",
    "        cf_matrices.append(cf_matrix)\n",
    "        print(f\"confusion matrix appended. epoch {epoch}\")\n",
    "        model.train()\n",
    "            \n",
    "        training_metrics = {}\n",
    "        training_metrics[\"labels\"] = valid_labels\n",
    "        training_metrics[\"preds\"] = valid_preds\n",
    "        training_metrics[\"mats\"] = cf_matrices\n",
    "\n",
    "    return training_loss, training_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f374c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i think the train loop is working:\n",
    "\n",
    "def train_rnn(model, train_dataset, test_dataset, num_epochs = 10, rate = 5):\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    training_losses, valid_losses, train_labels, train_preds, cf_matrices = [], [], [], [], []\n",
    "\n",
    "    model.init_optimizer()\n",
    "   \n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch_loss, train_epoch_acc, valid_epoch_loss, valid_epoch_acc = 0, 0, 0, 0\n",
    "\n",
    "        model.train()        # Set model to train mode\n",
    "\n",
    "        # Loop over the train data.\n",
    "        for i, (turns, winner, num_turns), in enumerate(tqdm(train_dataset, desc = \"Train Epoch %s\" % epoch)): ### turns = inputs, winner = labels\n",
    "\n",
    "            # Zero out gradients #\n",
    "            model.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass #\n",
    "            outputs = model(turns, num_turns)\n",
    "            \n",
    "            #loss = criterion(outputs, winner.float())\n",
    "            loss = criterion(outputs, torch.argmax(winner, dim=1))  # is this actually correct?\n",
    "            \n",
    "            # Backward pass #\n",
    "            loss.backward()\n",
    "            model.optimizer.step()\n",
    "            \n",
    "            # update training loss #\n",
    "            train_epoch_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            # Update training accuracy\n",
    "            preds = F.softmax(outputs, dim=1)\n",
    "            _, pred_labels = torch.max(preds, dim=1)\n",
    "            train_epoch_acc += torch.sum(pred_labels == torch.argmax(winner, dim=1)).item() \n",
    "            \n",
    "#             train_preds.append(winner.numpy())\n",
    "#             train_labels.append(winner)\n",
    "\n",
    "\n",
    "        train_loss = train_epoch_loss / len(train_dataset)\n",
    "        train_acc = train_epoch_acc / len(train_dataset)\n",
    "        training_losses.append(train_loss)\n",
    "        train_labels.append(winner)\n",
    "        train_preds.append(pred_labels)\n",
    "\n",
    "        \n",
    "        ###############\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        average_training_loss = train_epoch_loss / len(train_dataset)\n",
    "        average_training_acc = train_epoch_acc / len(train_dataset)\n",
    "        \n",
    "        training_losses.append(average_training_loss)\n",
    "        #train_labels.append(winner)\n",
    "        #train_preds.append(pred_labels)\n",
    "        \n",
    "        print(f\"epoch {epoch}, epoch_loss={train_loss}, epoch_acc={train_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
